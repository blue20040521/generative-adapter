{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data2/sm/generative-adapter_files/generative-adapter/src\")\n",
    "# Import functions to load the model, create an adaptor for LoRA weights,\n",
    "# and perform conditional text generation.\n",
    "\n",
    "from fastlora.config import FastLoraConfig\n",
    "from fastlora.model import FastLoraModelForCausalLM, FastLoraModel, get_peft_model_state_dict, set_peft_model_state_dict, load_pretrained_model\n",
    "import peft.peft_model as peft_model\n",
    "import peft.mapping as peft_mapping\n",
    "\n",
    "## monkey patching\n",
    "peft_model.PEFT_TYPE_TO_MODEL_MAPPING.update({\"FASTLORA\": FastLoraModel})\n",
    "peft_mapping.PEFT_TYPE_TO_CONFIG_MAPPING.update({\"FASTLORA\": FastLoraConfig})\n",
    "peft_model.get_peft_model_state_dict = get_peft_model_state_dict\n",
    "peft_model.set_peft_model_state_dict = set_peft_model_state_dict\n",
    "\n",
    "from fastlora.eval_utils import load_model_and_tokenizer\n",
    "from fastlora.eval_utils import fastlora_generate_adaptor\n",
    "from fastlora.eval_utils import fastlora_conditional_generate\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "from peft.config import PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# conda install /data2/sm/generative-adapter_files/generative-adapter/.conda  ipykernel --update-deps --force-reinstall\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the local cached model and the device to run on\n",
    "model_name_or_path = \"generative-adaptor/Generative-Adapter-Mistral-7B-Instruct-v0.2\"\n",
    "device = 'cuda'\n",
    "torch_dtype = torch.bfloat16\n",
    "attn_implementation = 'sdpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] we apply a monkey patch to the function `get_peft_model_state_dict` to support FastLora model\n",
      "missing_keys: []\n",
      "peft_model_state_dict.keys(): dict_keys(['base_model.model.model.layers.0.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.0.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.0.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.0.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.0.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.1.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.1.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.1.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.1.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.1.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.10.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.10.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.10.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.10.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.10.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.11.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.11.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.11.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.11.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.11.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.12.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.12.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.12.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.12.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.12.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.13.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.13.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.13.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.13.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.13.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.14.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.14.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.14.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.14.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.14.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.15.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.15.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.15.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.15.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.15.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.16.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.16.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.16.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.16.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.16.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.17.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.17.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.17.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.17.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.17.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.18.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.18.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.18.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.18.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.18.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.19.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.19.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.19.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.19.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.19.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.2.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.2.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.2.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.2.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.2.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.20.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.20.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.20.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.20.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.20.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.21.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.21.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.21.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.21.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.21.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.22.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.22.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.22.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.22.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.22.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.23.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.23.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.23.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.23.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.23.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.24.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.24.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.24.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.24.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.24.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.25.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.25.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.25.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.25.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.25.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.26.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.26.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.26.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.26.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.26.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.27.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.27.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.27.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.27.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.27.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.28.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.28.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.28.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.28.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.28.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.29.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.29.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.29.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.29.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.29.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.3.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.3.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.3.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.3.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.3.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.30.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.30.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.30.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.30.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.30.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.31.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.31.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.31.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.31.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.31.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.4.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.4.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.4.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.4.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.4.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.5.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.5.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.5.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.5.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.5.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.6.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.6.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.6.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.6.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.6.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.7.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.7.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.7.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.7.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.7.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.8.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.8.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.8.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.8.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.8.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight', 'base_model.model.model.layers.9.base_layer.self_attn.o_proj.fastlora_A1.weight', 'base_model.model.model.layers.9.base_layer.self_attn.o_proj.fastlora_A2.weight', 'base_model.model.model.layers.9.base_layer.self_attn.o_proj.fastlora_A3.weight', 'base_model.model.model.layers.9.base_layer.self_attn.o_proj.fastlora_B.weight', 'base_model.model.model.layers.9.base_layer.self_attn.o_proj.fastlora_hidden_state_norm.weight'])\n"
     ]
    }
   ],
   "source": [
    "# Load the PEFT configuration from the given pretrained model directory.\n",
    "peft_config = PeftConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Get the base model path from the configuration.\n",
    "base_model_path = peft_config.base_model_name_or_path\n",
    "\n",
    "# Ensure that the base model path is available.\n",
    "assert base_model_path is not None, \"base_model_name_or_path should not be None\"\n",
    "\n",
    "# Load the base causal language model from the retrieved base model path.\n",
    "# The model is loaded with a specific data type and attention implementation.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# Load the FastLora model for causal language modeling.\n",
    "# This model is built on top of the base model, uses the adapter settings from the PEFT config,\n",
    "# is not set for training, and is moved to the GPU.\n",
    "model = FastLoraModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    model_name_or_path,\n",
    "    adapter_name='default',\n",
    "    is_trainable=False,\n",
    "    config=peft_config,\n",
    ").cuda()\n",
    "\n",
    "# Load the tokenizer from the pretrained model directory.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt prefix that supplies context about a speaker's background.\n",
    "# This information may influence how the model generates the response.\n",
    "prompt_prefix = (\n",
    "    \"I volunteer in my spare time. I have been volunteering for 7 years. I volunteer in a homeless shelter in my town. \"\n",
    "    \"I'm not into cars. I wrestle for my day job. I like wrestling. I am not super into wrestling. I like crowds and meeting people. \"\n",
    "    \"I work out a few times each week when I need to be alone. I like country music a little bit. I like Taylor Swift. \"\n",
    "    \"I've lost fights recently. I work out a few times a week. I do not like working on cars. I am not patient. \"\n",
    "    \"I have two dogs: Baron Zemo and Spike. I have two older mustangs. I like vintage cars. I'm working on two Mustangs: a 68 and a 66 Hertz clone. \"\n",
    "    \"I've been working on cars since 1989. I have a Mustang convertible. I work on my car after work. I get frustrated working on my car sometimes. \"\n",
    "    \"I don't like crowds. I like working out. I like classic country. I am a dog trainer. My work keeps me busy.\"\n",
    ")\n",
    "\n",
    "# Define the input prompt that asks a question related to music.\n",
    "prompt_input = \"Hey, remember that time we talked about music? What was the artist you mentioned you could get into?\"\n",
    "\n",
    "# Set parameters for generating the LoRA weights and the text output.\n",
    "merge_strategy = 'concat'\n",
    "window_size = 1024\n",
    "max_new_tokens = 100\n",
    "stop = [\"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of context_input_ids: torch.Size([1, 1, 237])\n",
      "shape of context_attention_mask: torch.Size([1, 1, 237])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FastLoraLinear' object has no attribute 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate LoRA weights using the model, tokenizer, and prompt prefix.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This function adapts the model weights based on the given prompt context.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lora_weights \u001b[38;5;241m=\u001b[39m \u001b[43mfastlora_generate_adaptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Output the number of LoRA weights generated.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of LoRA weights generated:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(lora_weights))\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/sm/generative-adapter_files/generative-adapter/src/fastlora/eval_utils.py:352\u001b[0m, in \u001b[0;36mfastlora_generate_adaptor\u001b[0;34m(model, tokenizer, context_text, merge_strategy, max_window_size)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of context_input_ids: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of context_attention_mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_attention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 352\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m context_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m    357\u001b[0m lora_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_lora_weights(context_hidden_states, context_attention_mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1200\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1197\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:976\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    965\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    966\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    967\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         cache_position,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/sm/generative-adapter_files/generative-adapter/src/fastlora/model.py:599\u001b[0m, in \u001b[0;36mFastLoraDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m target_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_modules:\n\u001b[0;32m--> 599\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtarget_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    600\u001b[0m             B, S \u001b[38;5;241m=\u001b[39m target_module\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], target_module\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_segments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;66;03m# print(B, S, hidden_states.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/generative-adapter/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FastLoraLinear' object has no attribute 'args'"
     ]
    }
   ],
   "source": [
    "# Generate LoRA weights using the model, tokenizer, and prompt prefix.\n",
    "# This function adapts the model weights based on the given prompt context.\n",
    "lora_weights = fastlora_generate_adaptor(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt_prefix, \n",
    "    merge_strategy=merge_strategy, \n",
    "    max_window_size=window_size,\n",
    ")\n",
    "\n",
    "# Output the number of LoRA weights generated.\n",
    "print(\"Number of LoRA weights generated:\", len(lora_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lora_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate text using the model with the generated LoRA weights.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The function takes the input prompt and uses chat-style generation.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m output_text \u001b[38;5;241m=\u001b[39m fastlora_conditional_generate(\n\u001b[1;32m      4\u001b[0m     model, tokenizer, \n\u001b[1;32m      5\u001b[0m     input_text\u001b[38;5;241m=\u001b[39mprompt_input, \n\u001b[1;32m      6\u001b[0m     use_chat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m----> 8\u001b[0m     lora_weights\u001b[38;5;241m=\u001b[39m\u001b[43mlora_weights\u001b[49m, \n\u001b[1;32m      9\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     10\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print the generated text.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lora_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate text using the model with the generated LoRA weights.\n",
    "# The function takes the input prompt and uses chat-style generation.\n",
    "output_text = fastlora_conditional_generate(\n",
    "    model, tokenizer, \n",
    "    input_text=prompt_input, \n",
    "    use_chat=True,\n",
    "    mode=\"weights\", \n",
    "    lora_weights=lora_weights, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    stop=stop,\n",
    ")\n",
    "\n",
    "# Print the generated text.\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
